{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vanislekahuna/Statistical-Rethinking-PyMC/blob/main/Chp_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Statistical Rethinking: A Bayesian course with examples in Python**\n",
        "# **Ch 3 - Sampling the Imaginary**"
      ],
      "metadata": {
        "id": "yM4tLRu4OmKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Takeaways\n",
        "\n",
        "Jump to [*Section 3.4*](#scrollTo=0A8L-FZG6VA_)\n",
        "\n",
        "OR\n",
        "\n",
        "Read our article on [why most introductory examples of Bayesian Statistics misrepresent it](https://medium.com/towards-artificial-intelligence/why-most-introductory-examples-of-bayesian-statistics-misrepresent-it-d2e12ac69278)"
      ],
      "metadata": {
        "id": "OnblmgK9l3JV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://cdn.mos.cms.futurecdn.net/3kJhpErAArtBRJ8WnG8pNG.jpg.webp\" width=1200 height=600>\n",
        "\n",
        "[Source](https://www.cinemablend.com/news/2496306/people-are-loving-this-twilight-fans-reaction-to-finding-out-how-the-wolf-scenes-were-filmed)"
      ],
      "metadata": {
        "id": "-dxFyZRChA_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro\n",
        "\n",
        "Recap on the important features of Bayesian inference:\n",
        "- **Data**: The particular conjecture of outcomes that are presented to us.\n",
        "- **Likelihood**: The probability/likelihood of that particular conjecture of data, often represented as the **inverse probability** $P(E | H)$ in Bayes' Theorem. For example, the probability of observing 6 W's in 9 Tosses in the globe tossing experiment.\n",
        "- **Parameters**: The value we want to estimate, often representing the **prior** $P(H)$ in Bayes' Theorem. For example, going back to the Globe Tossing Experiment, we initialized our model with a 50/50 chance of the globe landing in either water or land. However, as we increase our sample size (let's say 1000 tosses), our model updates towards a parameter that more accurately represents the true ratio between water-to-land, which is roughly $0.71$ (i.e. our index finger will land on water in approximately 710 of those tosses).\n",
        "- **Marginal/Average Likelihood** $P(E)$: See below.\n",
        "- **Posterior**: This value is what we're trying to estimate for and represents the output of our model's learning process, which then becomes the *prior* in the next iteration.  \n",
        "\n",
        "Let's review Bayes' Theorem by using one of the most common examples of its application to medical testing from books introducing the concept. Below are the following *constants* in the hypothetical medical testing example that's often used in textbooks to illustrate Bayes' Theorem, <u>GIVEN</u> that the data has a **Binomial Distribution** (meaning there are only two possible outcomes):\n",
        "\n",
        "- The probability that the medical test can correctly identify someone with a given virus (i.e. the **statistical power** of the test): $P(Test^+ | Virus^+) = 0.95$\n",
        "- The amount of people in the population that actually carry the virus: $P(Virus^+) = 0.001 $\n",
        "- The probability of a false positive, which in other words, is the probability that the test diagnoses someone as being a virus carrier even though they are not: $P(Test^+ | Virus^-) = 0.01  $\n",
        "\n",
        "Now that we've received some information about the medical test, the question we want to answer is: *What is the probability that a person who carries the virus will actually test positive for it on the first attempt?*"
      ],
      "metadata": {
        "id": "SJUBVc-YOmHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bayes Theorem**:\n",
        "\n",
        "$ P(H | E) = \\frac{P(H) \\times P(E | H)}{P(E)} $\n",
        "\n",
        "<br>\n",
        "\n",
        "**Bayes Theorem** *given* the probability of the data/evidence is <u>UNKNOWN</u> and the data has a <u>**binomial distribution**</u>:\n",
        "\n",
        "$ P(H | E) = \\frac{P(H) \\times P(E | H)}{(P(H) \\cdot P(E | H)) + (P(H^-) \\cdot P(E | H^-))} $\n",
        "\n",
        "<br>\n",
        "\n",
        "Bayes Theorem Applied using the version where the probability of the data/evidence is *UNKNOWN* with a *BINOMIAL DISTRIBUTION*:\n",
        "\n",
        "$P(Virus^+ | Test^+) = \\frac{P(Virus^+) \\times P(Test^+ | Virus^+)}{(P(Virus^+) \\cdot P(Test^+ | Virus^+)) + (P(Virus^-) \\cdot P(Test^+ | Virus^-))} $\n",
        "\n",
        "$P(Virus^+ | Test^+) = \\frac{0.001 \\times 0.95}{(0.001 \\cdot 0.95) + (0.999 \\cdot 0.01)} $\n",
        "\n",
        "$P(Virus^+ | Test^+) =  \\frac{0.00095}{0.0000094905}$\n",
        "\n",
        "$P(Virus^+ | Test^+) =  0.08683729433272395 â‰ˆ 0.087$  "
      ],
      "metadata": {
        "id": "yuYJiB-W85oh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "likelihood = 0.95\n",
        "prior = 0.001\n",
        "false_pos = 0.01\n",
        "\n",
        "# Applying Bayes Theorem:\n",
        "\n",
        "posterior1 = (prior * likelihood) / ((prior * likelihood) + ((1-prior) * false_pos))\n",
        "print(f\"Numerator: {prior * likelihood}\")\n",
        "print(f\"Denominator: {(prior * likelihood) * ((1-prior) * false_pos)}\")\n",
        "print(f\"The new Posterior Probability: {posterior1}\")"
      ],
      "metadata": {
        "id": "Vq_bI3X07Gpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although our medical test is 95% accurate in detecting scenarios where a person carrying the virus tests positive (i.e. $P(Virus^+ | Test^+)$), we need to consider the other variables associated with the medical test. For example, we need to consider the prevalence of the virus (i.e. $P(Virus^+)$), which is our **prior probability**. Then, to gain a larger, more accurate view of the medical test, we need to evaluate the likelihood that a person is carrying the virus *given* that they've tested positive on the test (i.e. $P(Virus^+ | Test^+)$). In this case, $P(Virus^+ | Test^+)$ is still relatively low with an 8.7% chance of occurring because our prior was *VERY LOW* to begin with.\n",
        "\n",
        "If we want to increase the accuracy of our results, one solution is to test a person more than once and update the prior after each test. In this second round of testing, our new prior will be $P(Virus^+ | Test^+) = 0.087$ to reflect the posterior from our first test."
      ],
      "metadata": {
        "id": "zJoE28oK9wQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "posterior2 = (posterior1 * likelihood) / ((posterior1 * likelihood) + ((1- posterior1) * false_pos))\n",
        "\n",
        "print(f\"Numerator: {posterior1 * likelihood}\")\n",
        "print(f\"Denominator: {(posterior1 * likelihood) * ((1 - posterior1) * false_pos)}\")\n",
        "print(f\"The new Posterior Probability: {posterior2}\")"
      ],
      "metadata": {
        "id": "0-SirwqcC90A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from our application of Bayes Theorem, re-testing our patients results in a higher probability that they actually carry the virus from testing positive twice in a row. In other words, if a person were to test positive for a virus twice in a row, there's about a 90% chance they are indeed infected."
      ],
      "metadata": {
        "id": "ExCCOAfBOqqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And if we test for a third time and get a positive result, there's almost a 100% certainty that we have indeed contracted the virus."
      ],
      "metadata": {
        "id": "zzTxaJb3DPJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "posterior3 = (posterior2 * likelihood) / ((posterior2 * likelihood) + ((1- posterior2) * false_pos))\n",
        "\n",
        "print(f\"Numerator: {posterior2 * likelihood}\")\n",
        "print(f\"Denominator: {(posterior2 * likelihood) * ((1 - posterior2) * false_pos)}\")\n",
        "print(f\"The new Posterior Probability: {posterior3}\")"
      ],
      "metadata": {
        "id": "GZ_HSVzyDNI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install arviz==0.20.0\n",
        "!pip install matplotlib==3.8.0\n",
        "!pip install numpy==1.26.4\n",
        "!pip install pandas==2.2.2\n",
        "!pip install plotly==5.24.1\n",
        "!pip install pymc==5.19.1\n",
        "!pip install scipy==1.13.1"
      ],
      "metadata": {
        "id": "E7K3UXxFBMsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgB7-8RdPRJy"
      },
      "outputs": [],
      "source": [
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pymc as pm\n",
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWztm0lLPRJ3"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "# %load_ext watermark\n",
        "RANDOM_SEED = 8927\n",
        "np.random.seed(RANDOM_SEED)\n",
        "az.style.use(\"arviz-darkgrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why this generic example of Bayes Theorem sucks.\n",
        "\n",
        "Despite the common usage of medical testing as an example to illustrate how Bayes' Theorem works in practice, it can be challenging to grasp because of how the constants are reported as probabilities/percentages. There's nothing uniquely \"Bayesian\" about this example. The reason being is that **Bayesian Inference is distinguished by a broad use of probability, <u>NOT</u> the use of Bayes' Theorem.**\n",
        "\n",
        "Since all of the probabilities provided above reference frequencies of events rather than theoretical parameters, all major statistical philosophies would agree to use Bayes' Theorem in this case."
      ],
      "metadata": {
        "id": "Vug69LxwJHKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, it's harder for people to remember where each number/constant goes in the equation, which makes Bayesian inference seem harder than it has to be. To simplify things, let's reframe the problem in another context where we use counts/frequencies of events rather than their proportions:\n",
        "\n",
        "1. In a population of 100,000 people, 100 of them were vampires;\n",
        "2. Of the 100 people who are vampires, 95 of them will test positive for a vampire detection test;\n",
        "3. Of the 99,900 mortals, 999 of them will ALSO test positive in the vampire detection test.\n",
        "\n",
        "Now, if we were to ask the question: Of the people who test positive, what percentage of them are actually vampires? The answer to this requires us to add the vampires who tested positive (i.e. the True Positives $TP = 95$) with the normal people who also tested positive (i.e. the False Positives $FP = 999$) to get the total number of people who was identified as positive (i.e. Predicted Positive = $999 + 95 = 1094$). Now, if we divide the True Positives by total people who were Predicted Positive, that will represent the test's accuracy for correctly identifying vampires when tested only once:"
      ],
      "metadata": {
        "id": "KsxUx5ThJf6W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og-zp6_-PRJ4"
      },
      "source": [
        "#### Code 3.1\n",
        "\n",
        "$Pr(vampire|positive) = \\frac{Pr(positive|vampire) Pr(vampire)} {Pr(positive)}$\n",
        "\n",
        "$Pr(positive) = Pr(positive|vampire) Pr(vampire) + Pr(positive|mortal) (1 âˆ’ Pr(vampire))$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ Pr(vampire | positive) = \\frac{Pr(positive | vampire) \\cdot Pr(vampire)}{Pr(positive)} = \\frac{95}{(999 + 95)} = 0.087 $"
      ],
      "metadata": {
        "id": "YwxX0oXmkFP4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6S31awJPRJ5"
      },
      "outputs": [],
      "source": [
        "# Using proprortions\n",
        "PrPV = 0.95\n",
        "PrPM = 0.01\n",
        "PrV = 0.001\n",
        "PrP = PrPV * PrV + PrPM * (1 - PrV)\n",
        "PrVP = PrPV * PrV / PrP\n",
        "PrVP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using frequencies\n",
        "population = 100000\n",
        "vampires = 100\n",
        "\n",
        "vamp_pos = 95/vampires\n",
        "true_vamps_prior = vampires/population\n",
        "pos_mortals = 999 / (population - vampires) # 999 / 990000\n",
        "\n",
        "unstd_posterior = true_vamps_prior * vamp_pos #Num\n",
        "avg_likelihood = unstd_posterior + (pos_mortals * (1 - true_vamps_prior)) #Denom\n",
        "\n",
        "posterior = unstd_posterior / avg_likelihood\n",
        "posterior"
      ],
      "metadata": {
        "id": "QqBTz_A146xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our resulting answer is exactly the same as our previous example, $P(Virus^+ | Test^+) = 0.087$, only without the probabilies. To most people, the second example of *using counts instead of frequencies* is more intuitive because **natural frequencies** (also called the **frequency format**) are things we encounter in everyday life. We don't often encounter probabilities in the natural world.\n",
        "\n",
        "In this chapter, we'll follow a similar pattern of reframing by sampling **probability distributions** to produce counts which are a little more intuitive for people to understand. Posterior Distributions are Probability Distributions which means we can sample from them to produce *parameter values* from those sampled events. From a Bayesian perspective, parameter values are relative plausibilities, not physically random processes.\n",
        "\n",
        "Therefore, posteriors define the *expected frequency* of different parameter values appearing once we start plucking out parameters from the posterior. All this will make sense as we move along in the chapter."
      ],
      "metadata": {
        "id": "mwGEO5VQGhkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Section 3.1* - Sampling from a grid-approximate posterior\n",
        "\n"
      ],
      "metadata": {
        "id": "J68J1R8pusNE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLoNERsMPRJ6"
      },
      "source": [
        "#### Code 3.2 - 3.5\n",
        "\n",
        "We are going to use the same function we use on chapter 2 (code 2.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irPxGJ_KPRJ6"
      },
      "outputs": [],
      "source": [
        "def uniform_prior(grid_points):\n",
        "    \"\"\"\n",
        "    Returns Uniform (i.e. a flat) prior density (i.e. returns the number 5 in this case, x number of times)\n",
        "\n",
        "            Parameters:\n",
        "                grid_points (numpy.array): Array of prior values\n",
        "\n",
        "            Returns:\n",
        "                density (numpy.array): Uniform density of prior values\n",
        "    \"\"\"\n",
        "    return np.repeat(5, grid_points)\n",
        "\n",
        "\n",
        "def truncated_prior(grid_points, trunc_point=0.5):\n",
        "    \"\"\"\n",
        "    Returns Truncated prior density\n",
        "\n",
        "            Parameters:\n",
        "                grid_points (numpy.array): Array of prior values\n",
        "                trunc_point (double): Value where the prior is truncated\n",
        "\n",
        "            Returns:\n",
        "                density (numpy.array): Truncated density of prior values\n",
        "    \"\"\"\n",
        "    return (np.linspace(0, 1, grid_points) >= trunc_point).astype(int)\n",
        "\n",
        "\n",
        "def double_exp_prior(grid_points):\n",
        "    \"\"\"\n",
        "    Returns Double Exponential prior density\n",
        "\n",
        "            Parameters:\n",
        "                grid_points (numpy.array): Array of prior values\n",
        "\n",
        "            Returns:\n",
        "                density (numpy.array): Double Exponential density of prior values\n",
        "    \"\"\"\n",
        "    return np.exp(-5 * abs(np.linspace(0, 1, grid_points) - 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkrMn1_KPRJ8"
      },
      "outputs": [],
      "source": [
        "def binom_post_grid_approx(prior_func, grid_points=5, success=6, tosses=9):\n",
        "    \"\"\"\n",
        "    Returns the grid approximation of posterior distribution with a binomial likelihood.\n",
        "\n",
        "            Parameters:\n",
        "                    prior_func (function): A function that returns the likelihood of the prior\n",
        "                    grid_points (int): Number of points in the prior grid\n",
        "                    successes (int): Number of successes\n",
        "                    tosses (int): number of tosses\n",
        "\n",
        "            Returns:\n",
        "                    p_grid (numpy.array): Array of prior values\n",
        "                    posterior (numpy.array): Likelihood (density) of prior values\n",
        "    \"\"\"\n",
        "    # define grid\n",
        "    p_grid = np.linspace(0, 1, grid_points)\n",
        "\n",
        "    # define prior\n",
        "    prior = prior_func(grid_points)\n",
        "\n",
        "    # compute likelihood at each point in the grid\n",
        "    likelihood = stats.binom.pmf(success, tosses, p_grid)\n",
        "\n",
        "    # compute product of likelihood and prior\n",
        "    unstd_posterior = likelihood * prior\n",
        "\n",
        "    # standardize the posterior, so it sums to 1\n",
        "    posterior = unstd_posterior / unstd_posterior.sum()\n",
        "\n",
        "    return p_grid, posterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRkhY4NzPRJ8"
      },
      "outputs": [],
      "source": [
        "p_grid, posterior = binom_post_grid_approx(uniform_prior, grid_points=100, success=6, tosses=9)\n",
        "\n",
        "samples = np.random.choice(p_grid, p=posterior, size=int(1e4), replace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyMC Review on **Binomial Posterior Grid Approximation**:\n",
        "\n",
        "The `binom_post_grid_approx` function uses the \"`uniform_prior`\" function to return the number 5 $x$-number of times (in this case, we're returning it 100 times). The `p_grid` function returns 100 even inputs between 0 and 1 to serve as the prior to illustrate how the posterior probability of achieving the particular conjecture of data changes (6 Ws out 9 tosses) based on what our priors are. So, for example, each prior fed into our grid-approximate posterior is $x = [0.01, 0.02, 0.03...]$. Therefore, the function will return both the priors from `p_grid` AND the posterior distribution of sampling that particular conjecture of data.\n",
        "\n",
        "Once we've generated a list of posteriors from the uniform priors, we'll create a `samples` array from the `np.random.choice` function, which samples 1000 values (`size` argument) from the priors array (`a` argument) given each of the probabilities listed in the posterior (`p` arg).\n"
      ],
      "metadata": {
        "id": "5OjKFd0gBq-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"p_grid (priors): {p_grid[60:71]} \\n\\n \\\n",
        "posterior probability of the data: {posterior[60:71]} \\n\\n \\\n",
        "1000 Samples from the p_grid array, with the probability of each sample coming from each posterior value in the post array: \\n {samples[60:71]} \")\n",
        "\n",
        "plt.plot(p_grid, posterior)\n",
        "plt.ylabel(\"Posterior Probability\")\n",
        "plt.xlabel(\"Prior Probability / Parameter Values $p(water)$\")\n",
        "plt.title(f\"Grid Approximation Distribution\")"
      ],
      "metadata": {
        "id": "uBvYisjDztR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And in the graph below, we'll plot the density (i.e. the number of times that the particular posterior probability got sampled) of the samples to find that values around 0.61 were sampled the most from our `np.random.choice` function. This distribution mostly reflects the **Grid Approximate Distribution** graph generated in the previous code cell."
      ],
      "metadata": {
        "id": "pTQx17wIffKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Section 3.2* - Sampling to summarize\n",
        "\n",
        "With our model's help in generating a posterior distribution, our job in summarizing and interpreting the posterior distribution has just begun. A few questions you could ask about your data include:\n",
        "- How much of the posterior probability lies below some parameter value? This is a question about intervals of *defined boundaries*.\n",
        "- How much of the posterior distribution lies between two parameter values? So is this.\n",
        "- Which parameter value marks the lower 5% of the posterior probability? This is a question about intervals of *defined probability mass*.\n",
        "- Which range of parameter values contain 90% of the posterior probability? So is this.\n",
        "- Which parameter value has the highest posterior probability? This last question is a question about *point estimates*."
      ],
      "metadata": {
        "id": "eFwH08wjBp2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Figure 3.1 Sampling parameter values from the posterior distribution."
      ],
      "metadata": {
        "id": "g5KWaqCOpfgf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVombhjBPRJ9"
      },
      "outputs": [],
      "source": [
        "_, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "ax0.plot(samples, \"o\", alpha=0.2)\n",
        "ax0.set_xlabel(\"sample number\")\n",
        "ax0.set_ylabel(\"proportion water (p)\")\n",
        "az.plot_kde(samples, ax=ax1)\n",
        "ax1.set_xlabel(\"proportion water (p)\")\n",
        "ax1.set_ylabel(\"density\")\n",
        "\n",
        "plt.suptitle(\n",
        "    x=0.3,\n",
        "    y=-0.05,\n",
        "    t=\"Figure 3.1. Sampling parameter values from the posterior distribution \\n \\\n",
        "    Left: 10,000 samples from the posterior implied by the globe tossing data and model. \\n \\\n",
        "    Right: The density of samples (vertical) at each parameter value (horizontal).\",\n",
        "    ma=\"left\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l23MVOd_PRJ_"
      },
      "source": [
        "#### Code 3.6"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 **Intervals of defined boundaries.**\n",
        "\n",
        "If you wanted to know what was the posterior probability that the proportion of water on Earth is less than 0.5 using the *grid-approximate posterior*, you could find that out by adding up all the posterior probabilities of parameters/priors that are less than 0.5 which we'll find to be about 17%.\n",
        "\n",
        "Once we move beyond situations where Grid Approximation isn't as practical, such as when we have more than one parameter in the posterior distribution, this calculation won't be as simple."
      ],
      "metadata": {
        "id": "GveyVip2EE5D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8jVO0ezPRJ_"
      },
      "outputs": [],
      "source": [
        "sum(posterior[p_grid < 0.5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxgyjLHlPRKA"
      },
      "source": [
        "#### Code 3.7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, if we were to add up all the posterior probabilities that we *sampled* which were less than $p = 0.5$, we'll find a comparable result that's roughly 17%. However, keep in mind that the sum in this calculation may vary slightly because the samples we'll generate from the `np.random.choice()` function will change everytime. However, this method of <u>using samples from the posterior</u> does tend to generalize to more complex models with many parameters so you'll be able to use it in more situations."
      ],
      "metadata": {
        "id": "3sF_q4rPxB_x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqcwZKRQPRKA"
      },
      "outputs": [],
      "source": [
        "sum(samples < 0.5) / 1e4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PuGerZmPRKA"
      },
      "source": [
        "#### Code 3.8"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And if we use the same approach as we did in the last code cell by <u>**sampling from the grid-approximate posterior**</u>, let's investigate how much of the posterior probability lies between 0.5 and 0.75 of the prior probability:"
      ],
      "metadata": {
        "id": "yei18Hqsypmy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MsKpR5kPRKB"
      },
      "outputs": [],
      "source": [
        "sum((samples > 0.5) & (samples < 0.75)) / 1e4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2. Intervals of Defined Mass.\n",
        "\n",
        "Its most common for scientfic journals to reports something called **confidence intervals**, which in Bayesian terms is also known as **intervals of defined mass**. However in this text, we're going to rename this term as *compatibility interval* to get away from unwarranted implications of \"confidence\" or \"credibility\" [(Gelman & Greenland, 2019)](https://drive.google.com/file/d/1PQqpFwmTIP8XZOC00pJU4mQXfafkImpv/view). The main job of the **compatibility interval** is to communicate the shape of the distribution. And to do that, we could consider presenting a series of nested intervals that contain 67%, 89%, or 97% of the posterior distribution as an example, rather than presenting the interval which contains 95% of the distribution (i.e. $p <= 0.5$) which is what's most commonly reported in journals.\n",
        "\n",
        "However, its important to remember that the <u> **posterior distribution** is just an \"estimate\" which summarizes the plausabilities of each possible value in its parameter(s)</u>. Therefore, if choosing intervals leads to differences in inferences, then we're better off just plotting the entire distribution instead."
      ],
      "metadata": {
        "id": "QMWyTH1M0JnK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBL9c5V0PRKB"
      },
      "source": [
        "#### Code 3.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EldBa6uxPRKB"
      },
      "outputs": [],
      "source": [
        "np.percentile(samples, 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Section 3.1.1, we specified the proportion of samples that are within a certain **defined boundary**, such as the proportion of posterior distribution samples that are between the parameter values 0.5 and 0.75 (approx. 61% according to code 3.8) OR below 0.5 (approx. 17% according to code 3.7).\n",
        "\n",
        "With **intervals of defined mass** we're interested in the proportion of the resulting posterior probabilities that are within a certain percentile, using the `np.percentile(array, pctile)` function. For example in code 3.9, about 77% of the posterior distribution lies below the 80th percentile while in code 3.10, the posterior probability of the 10th percentile is around 0.45 while 0.82 represents the 90th percentile. Therefore, about 36% of the posterior distribution lies in between the 10th and 90th percentile, which we can also call a **percentile interval (PI)** for short."
      ],
      "metadata": {
        "id": "kwfRF7va5BDF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXGtuf7gPRKC"
      },
      "source": [
        "#### Code 3.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVmPIwXTPRKC"
      },
      "outputs": [],
      "source": [
        "percentile_interval = np.percentile(samples, [10, 90])\n",
        "percentile_interval"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "percentile_interval[1] - percentile_interval[0]"
      ],
      "metadata": {
        "id": "Cv7IitlK7KVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, in terms of supporting inferences about which parameters are consistent with the data, **percentile intervals (PI)** are not perfect. For example in Code 3.11, if we observe 3 waters in 3 tosses (i.e. landing on water every toss) in addition to a flat prior, we'll find that the posterior is highly skewed with its boundary at $p = 1$."
      ],
      "metadata": {
        "id": "xIYtxJkHqrkv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny_MD2FbPRKC"
      },
      "source": [
        "#### Code 3.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlYnVrldPRKD"
      },
      "outputs": [],
      "source": [
        "p_grid_skewed, posterior_skewed = binom_post_grid_approx(uniform_prior, grid_points=100, success=3, tosses=3)\n",
        "plt.plot(p_grid_skewed, posterior_skewed)\n",
        "plt.xlabel(\"proportion water (p)\")\n",
        "plt.ylabel(\"Density\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djao-HmBPRKD"
      },
      "source": [
        "#### Code 3.12"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And if we sample the posterior and identified the 75th percentile, supposedly 94% of the posterior distribution lie below it. This is incorrect! The reason being is that if we look at the graph, most of the distribution is centered on the parameter value $p = 1$ which is why **percentile intervals** can sometimes be misleading."
      ],
      "metadata": {
        "id": "w7vpbahtt9wr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSNafW9KPRKD"
      },
      "outputs": [],
      "source": [
        "samples_skewed = np.random.choice(p_grid_skewed, p=posterior_skewed, size=int(1e4), replace=True)\n",
        "np.percentile(samples_skewed, [25, 75])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3QQOLuMPRKE"
      },
      "source": [
        "#### Code 3.13"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast to percentile intervals, **highest posterior density intervals (HDPI)** which is designed to represent the parameter values that are most consistent with the data and returns an interval that's the most densest in the distribution, but also quite narrow. If we examine the results of the `az.hdi` function in Code 3.13, we'll see that region containing 50% of the posterior probability is between $p = 0.84$ and $p = 1$, which is also narrower in width than the method containing percentile interval (0.16 < 0.23).\n",
        "\n",
        "*(Note: This is probably why HDPI gets its own function in a Bayesian library instead of the percentile interval method where we have mannually input the 25th and 75th percentile using the* `np.percentile(samples_skewed, [25, 75])` *line)*"
      ],
      "metadata": {
        "id": "4dBkAwhhvrmR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdMgynwLPRKE"
      },
      "outputs": [],
      "source": [
        "az.hdi(samples_skewed, hdi_prob=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be fair however, **highest posterior density interval** has its weaknesses as well. For one, it's more taxing computationally and is also suffers from *simulation variance*, which is just another way of saying that it is sensitive to the amount of samples you draw from the posterior. It's also a bit harder to understand as compared to percentile intervals which many in the scientific or non-Bayesian audience already understand."
      ],
      "metadata": {
        "id": "2EoM1uWOyXFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Rethinking: What do compatibility intervals mean?**\n",
        "\n",
        "In non-Bayesian language, it's common to see statistical reports with a 95% \"confidence interval\" which is used to communicate that there is a 95% probability that the \"true\" parameter value lies within the interval. What these authors really mean is that if they repeated the study/analysis a large number of times, then 95% of the computed intervals would contain the \"true\" parameter value. Notice how I surrounded the word *true* in quotation marks because our models hardly every capture the true, objective parameter value in the *large* world that we're trying to draw inferences from. The only world where the true parameter value lies within a given interval 95% of the time is in the *small*, logical world that our golem lives and believes in. As inhabitants of the larger world, we're free to believe something else."
      ],
      "metadata": {
        "id": "JrNcJnQf-tzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.3. Point Estimates.\n",
        "\n",
        "Remember that *the Bayesian parameter distribution equates to an entire posterior distribution as a function that maps each unique parameter value to a plausable posterior value.* Therefore, if someone asked you to report a single number from that distribution (i.e. a **point estimate**), not only is it unnecessary but also harmful as it discards useful information.\n",
        "\n",
        "However, if YOU DID have to report a single point that summarizes the posterior, the most common value that scientists report is the *maximum a posteriori*. The **maximum a posteriori (MAP)** is the parameter value with the highest posterior probability, which is computed in Code 3.14 using the skewed distribution."
      ],
      "metadata": {
        "id": "575d77ZG-2dn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTMW-PIoPRKE"
      },
      "source": [
        "#### Code 3.14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP1Hrn6ZPRKE"
      },
      "outputs": [],
      "source": [
        "# Shows which parameter value is associated to the largest value in the posterior distribution.\n",
        "p_grid_skewed[posterior_skewed == max(posterior_skewed)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihYdx6O1PRKE"
      },
      "source": [
        "#### Code 3.15"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use the **mode** of the distribution to approximate the **maximum a posteriori**."
      ],
      "metadata": {
        "id": "hxn5AADdE7Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats.mode(samples_skewed)[0]"
      ],
      "metadata": {
        "id": "oeQjqmTdEsep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NsqVSLOPRKF"
      },
      "outputs": [],
      "source": [
        "print(f\" The parameter value, {round(float(stats.mode(samples_skewed)[0]), 3)} appeared most in the distribution \\\n",
        "at {float(stats.mode(samples_skewed)[1])} times and is therefore the mode.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MwAEfZ9PRKF"
      },
      "source": [
        "#### Code 3.16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the mode/maximum a posteriori, we can also consider the mean (0.803) or the median (0.848) of the distribution. But how do we choose?"
      ],
      "metadata": {
        "id": "dClcoKmQGRMa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7jj2Ys4PRKF"
      },
      "outputs": [],
      "source": [
        "np.mean(samples_skewed), np.median(samples_skewed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Q0N1PlPRKF"
      },
      "source": [
        "#### Code 3.17 - Using Loss Functions to pick a Point Estimate."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One principled way to go about choosing a single point is by using the minimal point in a *loss function*. A **loss function** is a rule that tells us the associated cost with using any particular point estimate. *Different loss functions imply different point estimates*. If we think of most loss functions as an inward $\\cup$, our goal is to get to the **global minimal value** which is the lowest part of the $\\cup$ and is also often the true value in most cases.\n",
        "\n",
        "$d$ = decision\n",
        "\n",
        "$p$ = The correct answer. In this case, we'll use our priors as the correct answer.\n",
        "\n",
        "In the graphs below, we've visualize what happens on each step of the loss function, $| d - p |$, we've decided on:\n",
        "\n",
        "`loss_func = sum(posterior * abs(decision - p_grid_skewed))`.\n",
        "\n",
        "The end result in Step 3 will be the ***weighted average loss*** *of the posterior loss distribution, where each loss is weighted by its corresponding posterior probability*. We'll first outline this process by setting our decision to $p = 0.5$ and then adding up all corresponding priors of the posterior probability distribution in order to get its weighted average loss:"
      ],
      "metadata": {
        "id": "AVVLfOrqGjWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Builing a plotting function for our convenience.\n",
        "def line_plot(x_coords, y_coords, x_label=None, y_label=None, title=None, subplot=plt):\n",
        "  subplot.plot(x_coords, y_coords)\n",
        "  subplot.set_xlabel(x_label)\n",
        "  subplot.set_ylabel(y_label)\n",
        "  subplot.set_title(title)"
      ],
      "metadata": {
        "id": "7JA8yrHmqetH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Visualizing every step of our loss function starting with the right hand side of this equation\n",
        "decision = 0.5\n",
        "loss_dist = abs(decision - p_grid_skewed)\n",
        "\n",
        "#STEP 2: Then including what it would look like if we multiplied the posterior distribution\n",
        "posterior_loss_dist = posterior_skewed * abs(0.5 - p_grid_skewed)\n",
        "\n",
        "# Visualizing both steps side-by-side\n",
        "_, (ax0, ax1) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "line_plot(p_grid_skewed,\n",
        "          loss_dist,\n",
        "          x_label=\"priors\",\n",
        "          y_label=\"loss distribution\",\n",
        "          subplot=ax0,\n",
        "          title=\"Step 1: abs(0.5 - p_grid)\")\n",
        "\n",
        "line_plot(p_grid_skewed,\n",
        "          posterior_loss_dist,\n",
        "          x_label=\"priors\",\n",
        "          y_label=\"posterior loss distribution\",\n",
        "          subplot=ax1,\n",
        "          title=\"Step 2: posterior * Step_1\")"
      ],
      "metadata": {
        "id": "evyx93ixr-Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRrS-5wMPRKF"
      },
      "outputs": [],
      "source": [
        "# STEP 3: Adding up all the values in the Posterior Loss Distribution\n",
        "sum(posterior * abs(0.5 - p_grid_skewed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfAkSWaVPRKG"
      },
      "source": [
        "#### Code 3.18"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is what our posterior loss distribution looks like if we graphed <u>every possible decision</u> to determine which one would minimize our loss and determined the decision that minimizes our loss in Code 3.19."
      ],
      "metadata": {
        "id": "U0OqQWlcuO1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1GL-d74PRKG"
      },
      "outputs": [],
      "source": [
        "weighted_avg_losses = [sum(posterior_skewed * abs(each_decision - p_grid_skewed)) for each_decision in p_grid_skewed]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code 3.19\n",
        "\n"
      ],
      "metadata": {
        "id": "q_5ppaWnvcKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_grid_skewed[weighted_avg_losses == min(weighted_avg_losses)]"
      ],
      "metadata": {
        "id": "2lBWy0vxvk8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_decision_loss = p_grid_skewed[weighted_avg_losses.index(min(weighted_avg_losses))]\n",
        "min_posterior_loss = weighted_avg_losses[weighted_avg_losses.index(min(weighted_avg_losses))]"
      ],
      "metadata": {
        "id": "2MX171h6SMfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, (ax2) = plt.subplots(1, 1)\n",
        "line_plot(p_grid_skewed,\n",
        "          weighted_avg_losses,\n",
        "          x_label=\"Skewed Priors\",\n",
        "          y_label=\"Posterior Loss Distribution\",\n",
        "          title=\"Plotting the loss of every possible decision in our loss function\",\n",
        "          subplot=ax2)\n",
        "\n",
        "min_loss_pt = plt.Circle((min_decision_loss, min_posterior_loss),\n",
        "                        0.01,\n",
        "                        color='r',\n",
        "                        fill=False)\n",
        "\n",
        "ax2.add_patch(min_loss_pt)"
      ],
      "metadata": {
        "id": "4LCHvvRyuJZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The parameter value (i.e. decision) which leads to a minimal loss is {min_decision_loss} \\n \\\n",
        "which equals to a posterior loss of {min_posterior_loss}\")"
      ],
      "metadata": {
        "id": "LoBkHeQQWXVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Surprisingly enough, the parameter value (0.8484) that leads to the smallest loss in our loss function is the SAME AS (or very close to) the **median** value in our sampled posterior distribution. But why is that?\n",
        "\n",
        "The reason being is that the median parameter value splits the posterior density in half such that half of the mass is above it and half of the mass is below it.\n",
        "\n",
        "Just a reminder, the loss function we've chosen is the commonly used **absolute loss** function, $| d - p |$, which leads to the median as the point estimate. Another loss function that's just as common is the **quadratic loss** function, $(d - p)^2$, which results in the posterior mean as the point estimate. When the posterior distribution is symmetrical and normal-looking, we'll often find that the median and the mean converge to the same point which means we can relax any anxiety we might have in choosing a loss function."
      ],
      "metadata": {
        "id": "7RhOyl-gWTTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Section 3.3* - Sampling to simulate prediction"
      ],
      "metadata": {
        "id": "SbRpfSMsfsqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another common use of sampling from the posterior distribution is to ease the **simulation** of our model's implied observation. Generating *implied observations* from a model is useful for 4 reasons:\n",
        "\n",
        "1. *Model design*. Not only can we sample from the posterior, but we can also sample from the prior. Therefore if we sample from the priors, we'll be able to see what the model expects before the data arrives which will help us better understand the implications of the prior distribution.\n",
        "\n",
        "2. *Model checking*. After a model us updated using data, it's worth simulating the implied observations, to check both whether the fit worked correctly and to investigate the model's behaviour.\n",
        "\n",
        "3. *Software validation*. In order to be sure that our model fitting software is working, its helpful to simulate observations under a known model, and then attempt to recover the values of the model's parameters using the simulated data.\n",
        "\n",
        "4. *Research design*. If you can simulate observations from your hypothesis, then you can evaluate whether the research design can be effective. In a narrow sense, this means doing a **power analysis**, but with the possibilities being much broader.\n",
        "\n",
        "5. ***Forecasting***. Estimates can be used to simulate new predictions, either for new cases or for future observations. These forecasts can be useful as an applied prediction, but also for criticizing and revising our model."
      ],
      "metadata": {
        "id": "6Cpd-Gibf2fV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.1. Dummy Data.\n",
        "\n",
        "Before we get into sampling for the purpose of simulating prediction, let's first review the purpose of the *likelihood function* in Bayes' Theorem. A **likelihood function** tells us how plausible a given observation is. Given only the parameters, the likelihood function can also define a distribution of possible observations that we can sample from, to simulate observation. In this way, Bayesian models have the quality of being *generative*, meaning that they're capable of simulation. For now, we'll use the term, **dummy data**, to describe data that we've simulated as a stand-in for actual data.\n",
        "\n",
        "As a reminder, the equation to describe the likelihood function for a binomial distibution where there's only two outcomes (outcome $o_{1}$ & outcome $o_{2}$), such as heads/tails or water/land is the following:\n",
        "\n",
        "$$Pr(o_{1}, o_{2} \\mid p) =  \\frac{(o_{1} + o_{2})!}{o_{1}!o_{2}!} p^{o_{1}} (1 âˆ’ p)^{o_{2}}$$"
      ],
      "metadata": {
        "id": "kBGAz7ZjkdD7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCEgsw5QPRKG"
      },
      "source": [
        "#### Code 3.20"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, suppose we've only observed $N = 2$ tosses in our globe example. This means there can be only 3 possible values generated from this outcome: either we observe 0, 1, or 2 tosses landing on water. Let's use the code in 3.20 to generate the plausabilities of observing each of those outcomes, using a prior of $p = 0.7$ which is close to the true proportion of water-to-land on Earth.\n"
      ],
      "metadata": {
        "id": "Phqi3d_UsDlO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Blw0PiWPRKG"
      },
      "outputs": [],
      "source": [
        "stats.binom.pmf(range(3), n=2, p=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the **likelihood function** function we've generated, we can interpret the results in the following way:\n",
        "\n",
        "- There's about a 9% chance that the globe lands on water 0 times out of 2 tosses;\n",
        "\n",
        "- A 42% chance that the globe lands on water once out of the 2 tosses;\n",
        "\n",
        "- And a 49% chance that the globe lands on water in each of the 2 tosses.\n",
        "\n",
        "In short, `scipy.stats.binom.pmf()` generates likelihoods, or proabilities, for a given conjecture of data (i.e. $P(E | H)$)."
      ],
      "metadata": {
        "id": "kK4uBsBquKTU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C19oApibPRKG"
      },
      "source": [
        "#### Code 3.21"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As opposed to generating probabilities for a given conjecture of data, instead we can also simulate observations using given probabilities by using the `binom.rvs` function in the `scipy.stats` package. Below is an example of how to use the function:"
      ],
      "metadata": {
        "id": "Ro5HElWJuv07"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIzXWubdPRKH"
      },
      "outputs": [],
      "source": [
        "stats.binom.rvs(n=2, p=0.7, size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that `rvs` stands for **random variates**. The key parameters for the `scipy.stats.binom.rvs()` function are as follows:\n",
        "\n",
        "- `n`: The number of possible outcomes. For example, if our argument for this parameter is $n=2$, this is equivalent to the numpy function: `np.arange(2) = [0, 1, 2]`\n",
        "- `p`: The probability of the generating the distribution. So for example, if we're generating the observations of a coin flip, we can expect $p$ to be $0.5$ meaning there's a 50/50 chance that the coin will land on either heads or tails. In our globe-tossing experiment, $p=0.7$ meaning that there's a 70/30 chance that globe will land on water, as opposed to land.\n",
        "- `size`: The number of observations we want to generate."
      ],
      "metadata": {
        "id": "IyAjNdmPxxHZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEyPDL9CPRKH"
      },
      "source": [
        "#### Code 3.22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKYp-eqaPRKH"
      },
      "outputs": [],
      "source": [
        "# Now let's simulate 10 observations using the random variates function\n",
        "stats.binom.rvs(n=2, p=0.7, size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Jn0BrsQPRKH"
      },
      "source": [
        "#### Code 3.23"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's generate 100,000 dummy observations and verify the proportion of each value (i.e. 0, 1, or 2) appearing based on the likelihoods we generated in Code 3.20:"
      ],
      "metadata": {
        "id": "PEV-YF16z44k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfFdIPSqPRKH"
      },
      "outputs": [],
      "source": [
        "dummy_w = stats.binom.rvs(n=2, p=0.7, size=int(1e5))\n",
        "[(dummy_w == i).mean() for i in range(3)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Looks* as though our simulated proportions match the likelihoods we calculated in Code 3.20 of each value appearing! The process of generating likelihoods and then simulating those observations and then comparing the proportions of each value appearing with their likelihoods is an example of **model checking**!"
      ],
      "metadata": {
        "id": "gU2MMz7U0YqH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gPemG3CPRKH"
      },
      "source": [
        "#### Code 3.24"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now instead of simulating the results from only 2 tosses of the globe, let's use the same `rvs` function to simulate 9 tosses similar to what we did with the in-person experiment. We'll repeat this experiment 100,000 times and use the true proportion of water-to-land as our $p$ argument."
      ],
      "metadata": {
        "id": "7TfjIdM_3SM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Figure 3.5. The results that appeared most frequently experiment."
      ],
      "metadata": {
        "id": "ljp0XLRuiONk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4j8kt8_PRKI"
      },
      "outputs": [],
      "source": [
        "dummy_w = stats.binom.rvs(n=9, p=0.7, size=int(1e5))\n",
        "bar_width = 0.1\n",
        "plt.hist(dummy_w, bins=np.arange(0, 11) - bar_width / 2, width=bar_width)\n",
        "plt.xlim(0, 9.5)\n",
        "plt.xlabel(\"# of times $p$ landed in water out of 9 tosses\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.suptitle(\n",
        "    x=1.32,\n",
        "    y=.55,\n",
        "    t=\"Figure 3.5. Distribution of simulated sample \\n \\\n",
        "    observations from 9 tosses of the globe. These \\n \\\n",
        "    samples assume the proportion of water is 0.7.\",\n",
        "    ma=\"left\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2. Model Checking.\n",
        "\n",
        "**Model Checking** means to:\n",
        "\n",
        "A) Ensure that the model fitting stage works correctly, and\n",
        "\n",
        "B) Evaluate the adequacy of a model for some purpose.\n",
        "\n",
        "\n",
        "Since Bayesian models are always **generative** by being able to simulate observations as well as estimate parameters from observations, once you condition a model on data, you can simulate to examine the model's empirical expectations."
      ],
      "metadata": {
        "id": "8jrB_G3U44s7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.2.1. *Did the software work?*\n",
        "\n",
        "In the simplest case, we can check whether the model worked by checking for correspondence between implied predictions and the data used to fit the model. Let's call this **retrodictions** because we're trying to get our golem to correctly *predict past data*. In other words, when we're generating **retrodictions** we're asking the model to reproduc the data used to educate it, which is what we did in Code 3.24.\n",
        "\n",
        "There's no way to be absolutely certain that the software works correcltly though even if our model reproduced the results we were expecting to see. There may have still been some subtle mistakes and when we move on to multi-level models, we'll start to expect a certain pattern of disconnect between retrodictions and observations. While this method is imperfect, we will be able to catch a lot of silly mistakes this way."
      ],
      "metadata": {
        "id": "W5hLeO-l52uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.3.2.2. *Is the model adequate?*\n",
        "\n",
        "After validating whether the data processing software worked as expected, its also useful to look into aspects of the data that were not well-described by the model's expectations. This process is not meant to check whether the model's assumptions are \"true,\" because all models are false. Rather, the goal is to assess exactly how the model fails to describe the data in order for us to better comprehend our model's results, make revisions, and improve its predictive capabilities.\n",
        "\n",
        "When we sample from the ENTIRE *posterior distribution*, rather than by picking a *point estimate* and drawing conlusions or running calculations from it, we'll be able to generate better results. The reason why its better to sample from the entire distribution rather than a point estimate is because there is a lot of information about **uncertainty** in the posterior distribution, which we'll lose from point estimates. The *uncertainty* that comes from the implied predictions in a model come about in 2 ways:\n",
        "\n",
        "1. **Observation uncertainty**. Just because you might know what the prior may be, for example in the context of a coin toss ($p = 0.5$) or a globe toss ($p = 0.71$), there is still uncertainty with what the next observation/toss will be.\n",
        "2. **Uncertainty with the prior**. Because there's an inherent uncertainty with the prior, the resulting posterior will inherit (or in other words, **propogate** - *carry it forward*) that uncertainty as well in addition to everything else that's dependent on the prior. This uncertainty with the prior will interact with the sampling variation when we're trying to assess what the model is telling us about the outcomes. Figure 3.6 in the textbook does an excellent job visualizing how we can capture/propogate a model's uncertainty by averaging over the posterior density for $p$, while computing the predictions:\n",
        "\n",
        "<ol>\n",
        "  <ol type=\"a\">\n",
        "    <li>For each possible value of $p$ there is an implied distribution of outcomes;</li>\n",
        "    <li>So if we compute the distribution of outcomes for each possible value of $p$, we can then average all of the outcomes together; </li>\n",
        "    <li> And lastly, generate a <b>posterior predictive distribution</b> using the <b>weighted average frequency</b> of each possible paramater value of $p$. </u>\n",
        "  </ol>\n",
        "</ol>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ncJihm8j8aEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code 3.25"
      ],
      "metadata": {
        "id": "6C51CPZEHQIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy_325 = stats.binom.rvs(n=9, p=0.6, size=int(1e4))"
      ],
      "metadata": {
        "id": "K3RkrRLDHKy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code 3.26"
      ],
      "metadata": {
        "id": "iz_LHFA-Hh7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To propogate this parameter uncertainty with these predictions, we can replace the `p` value with samples from the posterior (i.e. p_grid samples with posterior probabilities):"
      ],
      "metadata": {
        "id": "_icfdcWgm6cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_globe = stats.binom.rvs(n=9, p=samples)"
      ],
      "metadata": {
        "id": "FV--kOHZHhp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bar_width = 0.1\n",
        "plt.hist(dummy_globe, bins=np.arange(0, 11) - bar_width / 2, width=bar_width)\n",
        "plt.xlim(0, 9.5)\n",
        "plt.xlabel(\"Simulated # of times $p$ landed in water out of 9 tosses\")\n",
        "plt.ylabel(\"Frequency\")"
      ],
      "metadata": {
        "id": "q3C4uZiPI9h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we expect, the frequency of simulated observations we generated from the propabilities sampled from the globe toss' posterior distribution appear in proportion to their posterior probabilities. The simulated model predictions are quite consistent with the observed data in this case - the actual count of 6 lies right in the middle of the simulated distribution."
      ],
      "metadata": {
        "id": "hvSDrHJM0t2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Section 3.4* - Summary\n",
        "\n",
        "- A key difference between Frequentist Statistical Inference and Bayesian Statistical Inference is:\n",
        "\n",
        "  - Frequentist Inference is concerned with the data (hypothetical / data that they don't have yet) itself and procedural replicability. They're trying to accurately model how the broader population would look with the sample that they have. More fixed. Top-down in terms of data-producing scenarios.\n",
        "  - Bayesian Inference, on the other hand, is concerned with the data THEY DO HAVE, and they're primarily concerned with counting all the ways that the particular conjecture of data they have to work with could have been generated. This perspective is more comfortable with producing multiple scenarios producting the data which is takes the opposite approach of Frequentist inference in being bottom-up."
      ],
      "metadata": {
        "id": "0A8L-FZG6VA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises"
      ],
      "metadata": {
        "id": "tfV_SPwe4AA1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fqzdw_jPRKI"
      },
      "source": [
        "#### Code 3.27"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAuYmuFQPRKI"
      },
      "outputs": [],
      "source": [
        "p_grid, posterior = binom_post_grid_approx(uniform_prior, grid_points=100, success=6, tosses=9)\n",
        "np.random.seed(100)\n",
        "samples = np.random.choice(p_grid, p=posterior, size=int(1e4), replace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFSLCdHxPRKI"
      },
      "source": [
        "#### Code 3.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faaix3U1PRKI"
      },
      "outputs": [],
      "source": [
        "# fmt: off\n",
        "birth1 = np.array([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
        "                   1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1])\n",
        "birth2 = np.array([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
        "                   1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "                   1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
        "                   0, 0, 0, 1, 1, 1, 0, 0, 0, 0])\n",
        "# fmt: on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26kTtHCAPRKJ"
      },
      "source": [
        "#### Code 3.30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCjX1fnNPRKJ"
      },
      "outputs": [],
      "source": [
        "sum(birth1) + sum(birth2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2o2zKMCGr2w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l23MVOd_PRJ_",
        "JrNcJnQf-tzZ",
        "W5hLeO-l52uy",
        "ncJihm8j8aEp",
        "6fqzdw_jPRKI",
        "cFSLCdHxPRKI",
        "26kTtHCAPRKJ"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}